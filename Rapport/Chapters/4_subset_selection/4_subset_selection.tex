%!TEX root = ../../Main.tex
\graphicspath{{Chapters/Indledning/}}
%-------------------------------------------------------------------------------

\chapter{Subset selection}

This chapter will address the subject Subset selection. Subset selection concerns with selecting or shrinking the coefficients of features to make the model more interpretable and in some cases to predict better.

Linear models are simple and can be interpreted because it usually has a small number of coefficients.
In cases where the number of predictors is bigger than the number of samples, we can’t use the full least squares, because the solutions is not even defined. In such cases we must reduce the number of features to be able to obtain a solution. It is also very important to not fit your data too hard which regularize, or selection of features also helps with. Along the same lines, when we have a small number of features the model becomes more interpretable.  

There exist many different methods to perform subset selection. One of the methods to select the most important features regarding a specific response is called best subset selection. This is where we identify a subset of the predictors that is most related to the response. 

\section{Best subset selection}

Best subset selection algorithm is a simple algorithm used to understand which predictors are mostly linked to the responds. To perform best subset selection, we fit a separate least squares regression for each possible combination. It starts out by fitting all p models that contain exactly one predictor, then fitting all p models that contain exactly two predictors and so forth. The number of models to fit can be calculated like this: 
\begin{equation}
(\frac{p}{k})=\frac{p!}{k!(p-k)!}\label{Best_subset_models_eq}
\end{equation}
Where p is the number of all predictors, and k is the number of predictors in the subset. After all possible combinations of p models has been fitted, we then look at the resulting model, with the goal of identifying the best one. \\
Best subset selection algorithm can be divided into three steps:

\begin{enumerate}
	\item Let $M_0$ denote the \textit{null model}, which contain no predictors. This model simply predicts the sample mean for each observation.
	
	\item For k=1,2,...p:
	\begin{enumerate}
		\item Fit all $\binom{p}{k}$ models that contain exactly \textit{k} predictors.
		
		\item Pick the best among these $\binom{p}{k}$ models, and call it $M_k$. Here \textit{best} is defined as having the smallest RSS, or equivalently largest $R^2$
	\end{enumerate}
	\item Select a single best model from among $M_0$,...,$M_p$ using crass-validated prediction error, AIC, BIC or adjusted $R^2$
\end{enumerate}


The task of to selecting the best subset model, must be performed with care, because RSS decreases monotonically and the $R^2$ increases monotonically, as the number of features included in the models increases. So, if we use these statistics, we will always end up med the model involving all the variables. 
Another problem with a low RSS or a high $R^2$ is that it only indicates a model with a low training error. And a low training error does not equal a low test error. So, for those reasons we need to use cross-validation, AIC, BIC or adjusted $R^2$. 

\section{Choosing the optimal model}
As already explained RSS and $R^2$ are not suitable for selecting the best model among a collection of models, because these quantities are related to the training error and not the test error. 
So, in order to select the best model with respect to the test-error there are two common approaches:

\begin{enumerate}
	\item Indirectly estimate the test error by making an adjustment to the training error to account for the bias due to overfitting.
	
	\item Directly estimate the test error by either using a validation set approach or a cross-validation approach. 

\end{enumerate}

In this section we’ll only talk about AIC, BIC and adjusted $R^2$ which all are indirectly estimate of the test error.

\subsubsection{AIC}
AIC stand for Akaike information criterion and deals with the trade-off between goodness of fit of the model and simplicity of the model or it deals with the risk of overfitting and underfitting.

\begin{equation}
AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)\label{AIC_eq}
\end{equation}

The best model according to the AIC is where the AIC is smallest.

\subsubsection{BIC}
BIC stands for Bayesian information criterion. This is closely related to the AIC. Both the BIC and the AIC introduces a penalty term for number of parameters in the model to resolve the problem of overfitting. The penalty term is larger in the BIC than in the AIC. 
\begin{equation}
BIC=\frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)\label{BIC_eq}
\end{equation}
The best model according to the BIC is where the BIC is smallest. 


\subsubsection{Adjusted $R^2$}
Another very popular approach for selecting among a set of models that contain a different number of variables. $R^2$ is defined as 1-RSS/TSS, where TSS is the total sum of squares for the response. But as already mentioned $R^2$ keeps increasing as more variables are added to the model. The adjusted $R^2$ is calculated as:

\begin{equation}
Adjusted R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}\label{Adjusted_R2_eq}
\end{equation}
Unlike the AIC and BIC, a large value for adjusted $R^2$ indicates a model with a small test error. 